{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a002331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "\n",
    "# 문서를 로드 후 분할\n",
    "raw_documents = TextLoader(\"./test.txt\", encoding=\"utf-8\").load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# 문서에 대한 임베딩 생성\n",
    "embeddings_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "db = PGVector.from_documents(documents, embeddings_model, connection=connection)\n",
    "\n",
    "# 벡터 저장소에서 5개의 관련 문서 검색\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689d9f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_rag_fusion = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "하나의 입력 쿼리를 기반으로 여러 개의 검색 쿼리를 생성하는 유용한 어시스턴트입니다.\n",
    "다음과 관련된 여러 검색 쿼리를 영문으로 생성합니다: \n",
    "{question}\n",
    "\n",
    "출력(쿼리 4개):\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def parse_queries_output(message):\n",
    "    return message.content.split(\"\\n\")\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"gemma3:1b\", temperature=0)\n",
    "query_gen = prompt_rag_fusion | llm | parse_queries_output\n",
    "\n",
    "query = \"고대 그리스 철학사의 주요 인물은 누구인가요?\"\n",
    "\n",
    "generated_queries = query_gen.invoke(query)\n",
    "\n",
    "print(\"생성된 쿼리: \", generated_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece4ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "각 쿼리에 대한 관련 문서를 가져와 함수에 전달하여 관련 문서의 최종 목록을 재순위화(즉, 관련성에 따라 순서를 다시 지정)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"여러 순위 문서 목록에 대한 상호 순위 융합 및 RRF 공식에 사용되는 선택적 매개변수 k입니다.\"\"\"\n",
    "    # 사전을 초기화해 각 문서에 대한 융합된 점수를 보관합니다.\n",
    "    # 고유성을 보장하기 위해 문서가 콘텐츠별로 키로 만듭니다.\n",
    "    fused_scores = {}\n",
    "    documents = {}\n",
    "    for docs in results:\n",
    "        # 목록에 있는 각 문서를 순위(목록 내 위치)에 따라 반복\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = doc.page_content\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "                documents[doc_str] = doc\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "    # 융합된 점수를 기준으로 문서를 내림차순으로 정렬하여 최종 재순위 결과를 정리\n",
    "    reranked_doc_strs = sorted(\n",
    "        fused_scores, key=lambda d: fused_scores[d], reverse=True\n",
    "    )\n",
    "    return [documents[doc_str] for doc_str in reranked_doc_strs]\n",
    "\n",
    "\n",
    "retrieval_chain = query_gen | retriever.batch | reciprocal_rank_fusion\n",
    "\n",
    "result = retrieval_chain.invoke(query)\n",
    "\n",
    "print(\"순위를 사용해 검색한 컨텍스트: \", result[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b29d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "다음 컨텍스트만 사용해 질문에 답하세요.\n",
    "컨텍스트:{context}\n",
    "\n",
    "질문: {question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "query = \"고대 그리스 철학사의 주요 인물은 누구인가요?\"\n",
    "\n",
    "\n",
    "@chain\n",
    "def rag_fusion(input):\n",
    "    docs = retrieval_chain.invoke(input)\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "\n",
    "# 실행\n",
    "print(\"RAG 융합 실행\\n\")\n",
    "result = rag_fusion.invoke(query)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b1150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff0f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
