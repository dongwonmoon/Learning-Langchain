{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be1fb77e",
   "metadata": {},
   "source": [
    "### RRR(Rewrite-Retrieve-Read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b2d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "\n",
    "# 문서를 로드 후 분할\n",
    "raw_documents = TextLoader(\"../test.txt\", encoding=\"utf-8\").load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# 문서에 대한 임베딩 생성\n",
    "embeddings_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "db = PGVector.from_documents(documents, embeddings_model, connection=connection)\n",
    "\n",
    "# 벡터 저장소에서 2개의 관련 문서 검색\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01f59970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 관련 질문을 하기 전에 관련 없는 정보로 시작하는 쿼리\n",
    "query = \"일어나서 이를 닦고 뉴스를 읽었어요. 그러다 전자레인지에 음식을 넣어둔 걸 깜빡했네요. 고대 그리스 철학사의 주요 인물은 누구인가요?\"\n",
    "\n",
    "# 관련 문서 받아오기\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "print(docs[0].page_content)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09d2841c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='컨텍스트에 나와있는 정보는 없습니다.', additional_kwargs={}, response_metadata={'model': 'gemma3:1b', 'created_at': '2025-10-23T00:53:40.382544505Z', 'done': True, 'done_reason': 'stop', 'total_duration': 8619737645, 'load_duration': 2940083607, 'prompt_eval_count': 337, 'prompt_eval_duration': 5204626629, 'eval_count': 11, 'eval_duration': 455849747, 'model_name': 'gemma3:1b'}, id='run--852dffe3-adff-41e1-87fc-f7a265ba9983-0', usage_metadata={'input_tokens': 337, 'output_tokens': 11, 'total_tokens': 348})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"다음 컨텍스트만 사용해 질문에 답하세요.\n",
    "컨텍스트:{context}\n",
    "\n",
    "질문: {question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"gemma3:1b\", temperature=0)\n",
    "\n",
    "llm_chain = prompt | llm\n",
    "llm_chain.invoke({\"context\": docs, \"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "390090eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "재작성한 쿼리로 모델 호출\n",
      "\n",
      "재작성한 쿼리:  Okay, let's rephrase the question to be more effective for a search engine. Here's a revised query:\n",
      "\n",
      "**\"Ancient Greek philosopher known for his contributions to logic and metaphysics, and who is the primary figure associated with the invention of the electronic reheat?\"**\n",
      "\n",
      "Here's why this is better:\n",
      "\n",
      "* **Specificity:** It directly asks for a specific philosopher and their key contributions.\n",
      "* **Context:** It includes \"invention of the electronic reheat\" to narrow the search to relevant information.\n",
      "* **Keywords:** It uses keywords that search engines understand well (philosopher, logic, metaphysics, electronic reheat).\n",
      "\n",
      "Would you like me to refine this further, perhaps by adding a timeframe or focusing on a particular aspect of the philosopher?\n",
      "고대 그리스 철학사의 주요 인물은 사고를 깊이 하고, 세상의 본질을 탐구하며, 인간의 삶과 사회에 대한 통찰력을 제공한 **사고를 깊이 하고, 세상의 본질을 탐구하며, 인간의 삶과 사회에 대한 통찰력을 제공한 철학자**입니다.\n",
      "\n",
      "이 질문에 답하기 위해, 컨텍스트에서 제시된 정보를 바탕으로 다음과 같이 답변할 수 있습니다.\n",
      "\n",
      "*   **사고를 깊이 하고, 세상의 본질을 탐구하며, 인간의 삶과 사회에 대한 통찰력을 제공한 철학자**는 **소크라테스**입니다.\n",
      "\n",
      "컨텍스트에서 소크라테스는 \"Though primarily known for his ethical inquiries, Socrates laid the groundwork for critical philosophical inquiry through his emphasis on questioning and dialogue.\"라고 언급하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "rewrite_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "웹 검색 엔진이 주어진 질문에 답할 수 있도록 더 나은 영문 검색어를 제공하세요. 쿼리는 \\'**\\'로 끝내세요. \n",
    "\n",
    "질문: {x} \n",
    "\n",
    "답변:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def parse_rewriter_output(message):\n",
    "    return message.content.strip(\"'\").strip(\"**\")\n",
    "\n",
    "\n",
    "rewriter = rewrite_prompt | llm | parse_rewriter_output\n",
    "\n",
    "\n",
    "@chain\n",
    "def qa_rrr(input):\n",
    "    # 쿼리 재작성\n",
    "    new_query = rewriter.invoke(input)\n",
    "    print(\"재작성한 쿼리: \", new_query)\n",
    "    # 관련 문서 검색\n",
    "    docs = retriever.invoke(new_query)\n",
    "    # 프롬프트 포매팅\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # 답변 생성\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "\n",
    "print(\"\\n재작성한 쿼리로 모델 호출\\n\")\n",
    "\n",
    "# 재작성한 쿼리로 재실행\n",
    "result = qa_rrr.invoke(query)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c9962",
   "metadata": {},
   "source": [
    "### 다중 쿼리 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02283263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 쿼리를 위한 프롬프트\n",
    "perspectives_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"당신은 AI 언어 모델 어시스턴트입니다. 주어진 사용자 질문의 다섯 가지 버전을 생성하여 벡터 데이터베이스에서 관련 문서를 검색하세요. \n",
    "    사용자 질문에 대한 다양한 관점을 생성함으로써 사용자가 거리 기반 유사도 검색의 한계를 극복할 수 있도록 돕는 것이 목표입니다. \n",
    "    이러한 대체 질문을 개행으로 구분하여 제공하세요. \n",
    "    원래 질문: {question}\"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"gemma3:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59fa1b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_queries_output(message):\n",
    "    return message.content.split(\"\\n\")\n",
    "\n",
    "\n",
    "query_gen = perspectives_prompt | llm | parse_queries_output\n",
    "\n",
    "\n",
    "def get_unique_union(document_lists):\n",
    "    # 목록 여러 개를 포함한 리스트를 평탄화하고 중복 제거\n",
    "    deduped_docs = {\n",
    "        doc.page_content: doc for sublist in document_lists for doc in sublist\n",
    "    }\n",
    "    # 고유한 문서만 반환\n",
    "    return list(deduped_docs.values())\n",
    "\n",
    "\n",
    "retrieval_chain = query_gen | retriever.batch | get_unique_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bda1a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "다음 컨텍스트만 사용해 질문에 답하세요.\n",
    "컨텍스트:{context}\n",
    "\n",
    "질문: {question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "query = \"고대 그리스 철학사의 주요 인물은 누구인가요?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "720612fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다중 쿼리 검색\n",
      "\n",
      "컨텍스트에서 고대 그리스 철학사의 주요 인물은 다음과 같습니다.\n",
      "\n",
      "*   Phidias\n",
      "*   Polykleitos\n",
      "*   Myron\n",
      "*   Aristotle\n"
     ]
    }
   ],
   "source": [
    "@chain\n",
    "def multi_query_qa(input):\n",
    "    docs = retrieval_chain.invoke(input)\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "\n",
    "# run\n",
    "print(\"다중 쿼리 검색\\n\")\n",
    "result = multi_query_qa.invoke(query)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a15bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551eda91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e931e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0788f9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
