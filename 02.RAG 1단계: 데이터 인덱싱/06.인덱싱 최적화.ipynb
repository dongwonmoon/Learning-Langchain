{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e1948dc",
   "metadata": {},
   "source": [
    "### MultiVectorRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6254165a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of loaded docs:  624211\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.documents import Document\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "import uuid\n",
    "\n",
    "connection = \"postgresql+psycopg2://langchain:langchain@localhost:6024/langchain\"\n",
    "collection_name = \"summaries\"\n",
    "embeddings_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# 문서 로드\n",
    "loader = TextLoader(\"../test.txt\", encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(\"length of loaded docs: \", len(docs[0].page_content))\n",
    "\n",
    "# 문서 분할\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# 나머지 코드는 동일하게 유지\n",
    "prompt_text = \"다음 문서의 요약을 생성하세요:\\n\\n{doc}\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "llm = ChatOllama(temperature=0, model=\"gemma3:1b\")\n",
    "summarize_chain = {\"doc\": lambda x: x.page_content} | prompt | llm | StrOutputParser()\n",
    "\n",
    "summaries = summarize_chain.batch(chunks, {\"max_concurrency\": 5})\n",
    "\n",
    "# 벡터 저장소는 하위 청크를 인덱싱하는 데 사용\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings_model,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "# 상위 문서를 위한 스토리지 레이어\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# 원본 문서를 문서 저장소에 보관하면서 벡터 저장소에 요약을 인덱싱\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# 문서와 동일한 길이가 필요하므로 summaries에서 chunks로 변경\n",
    "doc_ids = [str(uuid.uuid4()) for _ in chunks]\n",
    "\n",
    "# 각 요약은 doc_id를 통해 원본 문서와 연결\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# 유사도 검색을 위해 벡터 저장소에 문서 요약을 추가\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "\n",
    "# doc_ids를 통해 요약과 연결된 원본 문서를 문서 저장소에 저장\n",
    "# 이를 통해 먼저 요약을 효율적으로 검색한 다음, 필요할 때 전체 문서를 가져옴\n",
    "retriever.docstore.mset(list(zip(doc_ids, chunks)))\n",
    "\n",
    "# 벡터 저장소가 요약을 검색\n",
    "sub_docs = retriever.vectorstore.similarity_search(\"chapter on philosophy\", k=2)\n",
    "\n",
    "print(\"sub docs: \", sub_docs[0].page_content)\n",
    "\n",
    "print(\"length of sub docs:\\n\", len(sub_docs[0].page_content))\n",
    "\n",
    "# retriever는 더 큰 원본 문서 청크를 반환\n",
    "retrieved_docs = retriever.invoke(\"chapter on philosophy\")\n",
    "\n",
    "print(\"length of retrieved docs: \", len(retrieved_docs[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1cc57c",
   "metadata": {},
   "source": [
    "### ColBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c83636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "import requests\n",
    "\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "\n",
    "def get_wikipedia_page(title: str):\n",
    "    \"\"\"\n",
    "    위키백과의 페이지를 불러온다.\n",
    "\n",
    "    :param title: str - 위키백과 페이지의 제목\n",
    "    :return: str - 페이지의 전체 텍스트를 raw 문자열로 반환\n",
    "    \"\"\"\n",
    "    # 위키백과 API 엔드포인트\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # API 요청을 위한 매개변수\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "    }\n",
    "\n",
    "    # 위키백과의 데이터를 받아올 헤더 설정\n",
    "    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1\"}\n",
    "\n",
    "    response = requests.get(URL, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    # 페이지 컨텐츠 추출\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return page[\"extract\"] if \"extract\" in page else None\n",
    "\n",
    "\n",
    "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")\n",
    "\n",
    "# 인덱스 생성\n",
    "RAG.index(\n",
    "    collection=[full_document],\n",
    "    index_name=\"Miyazaki-123\",\n",
    "    max_document_length=180,\n",
    "    split_documents=True,\n",
    ")\n",
    "\n",
    "# 쿼리\n",
    "results = RAG.search(query=\"What animation studio did Miyazaki found?\", k=3)\n",
    "\n",
    "print(results)\n",
    "\n",
    "# 랭체인에 전달\n",
    "retriever = RAG.as_langchain_retriever(k=3)\n",
    "retriever.invoke(\"What animation studio did Miyazaki found?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
